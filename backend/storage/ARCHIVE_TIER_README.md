# Azure Archive Tier Support - Integration Complete ‚úÖ

## Files Added/Modified

### New Files
- ‚úÖ `azure_blob_archive.py` - Enhanced Azure provider with archive tier support

### Modified Files
- ‚úÖ `config.py` - Enhanced with archive tier settings
- ‚úÖ `factory.py` - Enhanced to support archive-enabled provider
- ‚úÖ `.env.example` - Updated with archive tier configuration

---

## Quick Start

### 1. Update Your .env File

Add these settings to your `.env` file:

```env
# Storage Provider
STORAGE_PROVIDER=azure
STORAGE_AZURE_CONNECTION_STRING=your_connection_string
STORAGE_AZURE_CONTAINER_NAME=pixcrawler-storage

# Archive Tier Settings
STORAGE_AZURE_ENABLE_ARCHIVE_TIER=true
STORAGE_AZURE_DEFAULT_TIER=hot
STORAGE_AZURE_REHYDRATE_PRIORITY=standard
```

### 2. Use in Your Code

```python
from storage.factory import create_storage_provider
from storage.config import StorageSettings

# Create provider (works exactly as before)
settings = StorageSettings()
provider = create_storage_provider(settings)

# Existing operations work unchanged
provider.upload("image.jpg", "images/image.jpg")
provider.download("images/image.jpg", "local.jpg")
provider.delete("images/image.jpg")
```

### 3. Use Archive Features (NEW)

```python
from storage.azure_blob_archive import AccessTier

# Upload to Archive tier (94% cost savings!)
provider.upload("backup.tar.gz", "backups/backup.tar.gz", 
               tier=AccessTier.ARCHIVE)

# Archive old files
provider.archive_blob("images/old_photo.jpg")

# List archived files
archived = provider.list_blobs_by_tier(tier=AccessTier.ARCHIVE)

# Get file info with tier
info = provider.get_blob_info("images/photo.jpg")
print(f"Tier: {info['tier']}")
```

---

## Cost Savings

| Tier | Cost/GB/Month | 1 TB/Year | Savings |
|------|---------------|-----------|---------|
| Hot | $0.018 | $216 | - |
| Cool | $0.010 | $120 | 44% |
| Archive | $0.001 | $12 | 94%! |

---

## New Methods Available

When `STORAGE_AZURE_ENABLE_ARCHIVE_TIER=true`:

- `archive_blob(blob_name)` - Move to Archive tier
- `rehydrate_blob(blob_name, target_tier, priority)` - Bring back from Archive
- `set_blob_tier(blob_name, tier)` - Change tier
- `get_blob_info(blob_name)` - Get detailed info with tier
- `list_blobs_by_tier(tier, prefix)` - List by tier

---

## PixCrawler Use Cases

### Archive Old Crawls
```python
from datetime import datetime, timedelta

cutoff = datetime.now() - timedelta(days=90)
old_crawls = get_old_crawls(cutoff)

for crawl in old_crawls:
    provider.archive_blob(crawl)
```

### Compliance Storage
```python
provider.upload("compliance.png", "compliance/2024/screenshot.png",
               tier=AccessTier.ARCHIVE,
               metadata={"retention": "10years"})
```

---

## Important Notes

‚ö†Ô∏è **Archive Tier Limitations:**
- Retrieval time: Up to 15 hours (Standard) or < 1 hour (High priority)
- Minimum storage: 180 days
- Cannot download directly - must rehydrate first
- Only use for rarely accessed data

‚úÖ **Backward Compatibility:**
- All existing code works unchanged
- No breaking changes
- Optional features

---

## Documentation

For complete integration guide, see:
`D:\DEPI\python tasks\pixcrawel project\Azure Implement Blob Archive Tier Support\pixcrawler_integration\`

- `PIXCRAWLER_INTEGRATION_GUIDE.md` - Complete guide
- `integration_guide.html` - Visual guide
- `.env.pixcrawler.example` - Full configuration template

---

**Ready to save 94% on storage costs! üöÄ**
